{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNBBiIMnEijZ88aZ/mxYdb2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a7c7d308454a4b62841f10eaf5b62251":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b64ff15803c4fdbb8f3e0063aa3d556","IPY_MODEL_304817cd51f34112b8a638c64eae240d","IPY_MODEL_a101c9a36d584b8f9bb74b5391738f47"],"layout":"IPY_MODEL_01907a7d96c645cd87b3cb1ed2afbecd"}},"5b64ff15803c4fdbb8f3e0063aa3d556":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_544034fcd88a4814a03cb7354240dff0","placeholder":"​","style":"IPY_MODEL_68300ee3fa684a5e83b210bdf2fa3534","value":"tokenizer_config.json: 100%"}},"304817cd51f34112b8a638c64eae240d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8f37db40cb44ed1adf9e0f1aa2d96b4","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_853e7be233c2408590377373932f6d30","value":2}},"a101c9a36d584b8f9bb74b5391738f47":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9b01b2ab9be4717b93a8279d8283207","placeholder":"​","style":"IPY_MODEL_5456187ed2834838853faa0705fcfcfc","value":" 2.00/2.00 [00:00&lt;00:00, 130B/s]"}},"01907a7d96c645cd87b3cb1ed2afbecd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"544034fcd88a4814a03cb7354240dff0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68300ee3fa684a5e83b210bdf2fa3534":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8f37db40cb44ed1adf9e0f1aa2d96b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"853e7be233c2408590377373932f6d30":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a9b01b2ab9be4717b93a8279d8283207":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5456187ed2834838853faa0705fcfcfc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a32ce732bdf94eedba46bbb7f4e95180":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eeb6c3edb4a54288a3aae07c3b2aedac","IPY_MODEL_8aad0229d9e044c7b0ae3a7a0f5e5544","IPY_MODEL_579fa3d14aac422db5f4abf5fb904b8b"],"layout":"IPY_MODEL_e1e9731a30c94545a6707e0c4e973708"}},"eeb6c3edb4a54288a3aae07c3b2aedac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68749d33967f452dbc19c78de79cac40","placeholder":"​","style":"IPY_MODEL_c17d9c43744a48f9a832f71b7eaff513","value":"config.json: 100%"}},"8aad0229d9e044c7b0ae3a7a0f5e5544":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_41ba949426c2475f9dceec60db6b07cb","max":1534,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5b4731d282d4767998a4d3044ff826e","value":1534}},"579fa3d14aac422db5f4abf5fb904b8b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_618d0e9d6d9c47ddae3c8ab2ba4a5d53","placeholder":"​","style":"IPY_MODEL_a20705ec11f24557a176ab1bbcdd5547","value":" 1.53k/1.53k [00:00&lt;00:00, 96.9kB/s]"}},"e1e9731a30c94545a6707e0c4e973708":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68749d33967f452dbc19c78de79cac40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c17d9c43744a48f9a832f71b7eaff513":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41ba949426c2475f9dceec60db6b07cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5b4731d282d4767998a4d3044ff826e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"618d0e9d6d9c47ddae3c8ab2ba4a5d53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a20705ec11f24557a176ab1bbcdd5547":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a29d38b114fa48a3b3ddc781ad14eec8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_badb1228d20049288101cb6152539e06","IPY_MODEL_7c43704a4d3d433496a4dfd7aa59a540","IPY_MODEL_50bfef63237a4a5b87232cf68b2df515"],"layout":"IPY_MODEL_0866f1eff30446e280e0a2a7f5547404"}},"badb1228d20049288101cb6152539e06":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b36c1f83c1624641b7307e20f4509d79","placeholder":"​","style":"IPY_MODEL_15452b96a92c4d0fb4d7cb4b712af51e","value":"vocab.txt: 100%"}},"7c43704a4d3d433496a4dfd7aa59a540":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0292e26b7ac4e24a5fcbb35df1f8241","max":229167,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f2918279ecca4c8ebffcbab92ed8e230","value":229167}},"50bfef63237a4a5b87232cf68b2df515":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da460b6706eb4bef9c1a6216697450c5","placeholder":"​","style":"IPY_MODEL_f5356351b3db4123aa5c49248e829bb5","value":" 229k/229k [00:00&lt;00:00, 3.66MB/s]"}},"0866f1eff30446e280e0a2a7f5547404":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b36c1f83c1624641b7307e20f4509d79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15452b96a92c4d0fb4d7cb4b712af51e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0292e26b7ac4e24a5fcbb35df1f8241":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2918279ecca4c8ebffcbab92ed8e230":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da460b6706eb4bef9c1a6216697450c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5356351b3db4123aa5c49248e829bb5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"272ebbba3b2045f490b4f6ca44a168f8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fea31b9cc7014200a97acd3b257a5ff0","IPY_MODEL_317e81a28cf84116bb1f7b599c0e085e","IPY_MODEL_d5a22dd934d645df811f173b741ef273"],"layout":"IPY_MODEL_fa2edb46309c47adaf6ce87fffbd243a"}},"fea31b9cc7014200a97acd3b257a5ff0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23b32784f98f4892bc40cec5524dbcf3","placeholder":"​","style":"IPY_MODEL_cfd9e2f60e2e4621bdee13fd20dda261","value":"special_tokens_map.json: 100%"}},"317e81a28cf84116bb1f7b599c0e085e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a82dab2de32a43b5a1f64762a5f8adb8","max":112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db3e5d7440de43f58592ce3b64272aca","value":112}},"d5a22dd934d645df811f173b741ef273":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08973217de124726b42c6ef576aa6376","placeholder":"​","style":"IPY_MODEL_499367cbb5fe42caa03c4201384ab8e4","value":" 112/112 [00:00&lt;00:00, 6.49kB/s]"}},"fa2edb46309c47adaf6ce87fffbd243a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23b32784f98f4892bc40cec5524dbcf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfd9e2f60e2e4621bdee13fd20dda261":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a82dab2de32a43b5a1f64762a5f8adb8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db3e5d7440de43f58592ce3b64272aca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"08973217de124726b42c6ef576aa6376":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"499367cbb5fe42caa03c4201384ab8e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfd81693b04d449b8682f5f7e8e4834c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b9633043a6ea43d4a80d7079110b9ae8","IPY_MODEL_6d550dd5d73a4611a901a30e5f6b8f46","IPY_MODEL_d4a4bdee6da346e081a7a046c8a7d813"],"layout":"IPY_MODEL_d1e965612121401fbbd736937b3be73a"}},"b9633043a6ea43d4a80d7079110b9ae8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c626689b46164abbb11022c7da16d9e8","placeholder":"​","style":"IPY_MODEL_afc4a499dcc64998ada0a9ebe2be5791","value":"tf_model.h5: 100%"}},"6d550dd5d73a4611a901a30e5f6b8f46":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2dbf9c303e594cffbc8bd7c7bdbd7440","max":655811760,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b3b3b826a5a947de89ab614dae170bc4","value":655811760}},"d4a4bdee6da346e081a7a046c8a7d813":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a4e607e49434677897065ca4e11fd7a","placeholder":"​","style":"IPY_MODEL_f08e00e5d2994daa9c47f7d0b5495112","value":" 656M/656M [00:03&lt;00:00, 246MB/s]"}},"d1e965612121401fbbd736937b3be73a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c626689b46164abbb11022c7da16d9e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afc4a499dcc64998ada0a9ebe2be5791":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2dbf9c303e594cffbc8bd7c7bdbd7440":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3b3b826a5a947de89ab614dae170bc4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a4e607e49434677897065ca4e11fd7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f08e00e5d2994daa9c47f7d0b5495112":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"WTuc20imDTzb","executionInfo":{"status":"ok","timestamp":1749795000889,"user_tz":-420,"elapsed":51261,"user":{"displayName":"ISNAN WIJAYAKUSUMA","userId":"13733359583425132025"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ea196572-cff5-4a57-85ea-bf65737fb369"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from transformers import TFAutoModel, AutoTokenizer\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.utils.class_weight import compute_class_weight\n","import tensorflow.data as tf_data\n","from tensorflow.keras import layers\n","import re\n","import pickle\n","import json\n","\n","#Mount ke googledrive untuk akses data\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["class IndoBERTHoaxClassifier:\n","    def __init__(self, model_name='indobenchmark/indobert-base-p1', max_length=128):\n","        \"\"\"\n","        IndoBERT Fine-Tuning classifier\n","        Args:\n","        model_name : IndoBERT model identifier\n","        max_length: Max sequence length\n","        \"\"\"\n","        self.model_name = model_name\n","        self.max_length = max_length\n","        self.tokenizer = None\n","        self.bert_model = None\n","        self.model = None\n","\n","        # Initialize tokenizer\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","    def preprocess_text(self, text, minimal_preprocessing=True):\n","        # Text preprocessing\n","        if pd.isna(text) or not text:\n","            return \"\"\n","\n","        text = str(text).strip()\n","\n","        if minimal_preprocessing:\n","            # for already cleaned data\n","            text = re.sub(r'\\s+', ' ', text).strip()\n","            return text if len(text) > 3 else \"\"\n","        else:\n","            # Additional cleaning if needed\n","            text = text.lower()\n","            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n","            text = re.sub(r'@\\w+|#\\w+', '', text)\n","            text = re.sub(r'\\s+', ' ', text).strip()\n","            return text\n","\n","    def balance_dataset(self, df, text_column='text_akhir', label_column='label'):\n","        # Balance the dataset\n","        print(\"Dataset distribution before balancing:\")\n","        print(df[label_column].value_counts())\n","\n","        class_0 = df[df[label_column] == 0]\n","        class_1 = df[df[label_column] == 1]\n","\n","        min_size = min(len(class_0), len(class_1))\n","\n","        # Check if min_size is zero, if so, return an empty DataFrame\n","        if min_size == 0:\n","            print(\"Warning: One of the classes is empty, returning empty DataFrame after balancing.\")\n","            return pd.DataFrame(columns=df.columns)\n","\n","        class_0_balanced = class_0.sample(n=min_size, random_state=42)\n","        class_1_balanced = class_1.sample(n=min_size, random_state=42)\n","\n","        balanced_df = pd.concat([class_0_balanced, class_1_balanced])\n","        balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n","        print(\"Dataset distribution after balancing:\")\n","        print(balanced_df[label_column].value_counts())\n","\n","        return balanced_df\n","\n","    def tokenize_texts(self, texts):\n","        \"\"\"Tokenize texts using IndoBERT tokenizer\"\"\"\n","        # Check if texts is empty before calling tolist()\n","        if texts.empty:\n","            print(\"Warning: Input texts are empty during tokenization.\")\n","            return {'input_ids': tf.constant([], dtype=tf.int32),\n","                    'attention_mask': tf.constant([], dtype=tf.int32)}\n","\n","        encoded = self.tokenizer(\n","            texts.tolist(),\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='tf',\n","            return_attention_mask=True,\n","            verbose=False  # Changed to False to reduce output\n","        )\n","\n","        return {\n","            'input_ids': encoded['input_ids'],\n","            'attention_mask': encoded['attention_mask']\n","        }\n","\n","    def build_model(self):\n","        \"\"\"Build IndoBERT model using subclassing approach\"\"\"\n","\n","        # Create a custom model class that properly integrates BERT\n","        class IndoBERTClassifier(tf.keras.Model):\n","            def __init__(self, model_name, max_length, **kwargs):\n","                super(IndoBERTClassifier, self).__init__(**kwargs)\n","                self.max_length = max_length\n","                self.bert_model = TFAutoModel.from_pretrained(model_name)\n","                self.dropout1 = layers.Dropout(0.3)\n","                self.dense1 = layers.Dense(256, activation='relu', name='dense_1')\n","                self.dropout2 = layers.Dropout(0.2)\n","                self.dense2 = layers.Dense(128, activation='relu', name='dense_2')\n","                self.dropout3 = layers.Dropout(0.1)\n","                self.classifier = layers.Dense(1, activation='sigmoid', name='classifier')\n","\n","            def call(self, inputs, training=None):\n","                input_ids = inputs['input_ids']\n","                attention_mask = inputs['attention_mask']\n","\n","                # Pass through BERT\n","                bert_output = self.bert_model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    training=training\n","                )\n","\n","                # Use CLS token (first token) for classification\n","                pooled_output = bert_output.last_hidden_state[:, 0, :]\n","\n","                # Classification layers\n","                x = self.dropout1(pooled_output, training=training)\n","                x = self.dense1(x)\n","                x = self.dropout2(x, training=training)\n","                x = self.dense2(x)\n","                x = self.dropout3(x, training=training)\n","                outputs = self.classifier(x)\n","\n","                return outputs\n","\n","        # Create the model\n","        self.model = IndoBERTClassifier(\n","            model_name=self.model_name,\n","            max_length=self.max_length,\n","            name='IndoBERT_Classifier'\n","        )\n","\n","        # Build the model by calling it with dummy input\n","        dummy_input = {\n","            'input_ids': tf.zeros((1, self.max_length), dtype=tf.int32),\n","            'attention_mask': tf.zeros((1, self.max_length), dtype=tf.int32)\n","        }\n","        _ = self.model(dummy_input)\n","\n","        return self.model\n","\n","    def prepare_data(self, csv_path, text_column='text_akhir', label_column='label'):\n","        \"\"\"Prepare data for training\"\"\"\n","        # Load data\n","        print(f\"Loading data from {csv_path}\")\n","        try:\n","            df = pd.read_csv(csv_path)\n","        except FileNotFoundError:\n","            print(f\"Error: File not found at {csv_path}\")\n","            return pd.Series(dtype=str), pd.Series(dtype=int)  # Return empty Series\n","\n","        print(f\"Original dataset shape: {df.shape}\")\n","\n","        # Remove missing values\n","        df = df.dropna(subset=[text_column, label_column])\n","        print(f\"Shape after dropping NA: {df.shape}\")\n","\n","        # Balance dataset\n","        df = self.balance_dataset(df, text_column, label_column)\n","        print(f\"Shape after balancing: {df.shape}\")\n","\n","        # Preprocess text (minimal since already preprocessed)\n","        df[text_column] = df[text_column].apply(lambda x: self.preprocess_text(x, True))\n","        print(f\"Shape after preprocessing text: {df.shape}\")\n","\n","        # Remove empty texts\n","        df = df[df[text_column].str.len() > 0]\n","        print(f\"Shape after removing empty texts: {df.shape}\")\n","\n","        print(f\"Final dataset shape: {df.shape}\")\n","\n","        if df.empty:\n","            print(\"Warning: Final dataset is empty.\")\n","            return pd.Series(dtype=str), pd.Series(dtype=int)  # Return empty Series\n","\n","        return df[text_column], df[label_column]\n","\n","    def train(self, csv_path, validation_split=0.2, epochs=5, batch_size=16, learning_rate=2e-5):\n","        \"\"\"Fine-tune IndoBERT for hoax classification\"\"\"\n","\n","        # Prepare data\n","        print(\"Preparing data\")\n","        texts, labels = self.prepare_data(csv_path)\n","\n","        # Check if data is empty\n","        if texts.empty or labels.empty:\n","            print(\"Error: No data available for training after preparation.\")\n","            return None\n","\n","        # Split data\n","        if len(texts) < 2 or len(labels) < 2:\n","            print(\"Error: Not enough data samples to perform train-validation split.\")\n","            return None\n","\n","        # Ensure stratify is possible if classes are present\n","        if len(np.unique(labels)) > 1:\n","            X_train, X_val, y_train, y_val = train_test_split(\n","                texts,\n","                labels,\n","                test_size=validation_split,\n","                random_state=42,\n","                stratify=labels\n","            )\n","        else:\n","            print(\"Warning: Only one class present in the dataset. Skipping stratification.\")\n","            X_train, X_val, y_train, y_val = train_test_split(\n","                texts,\n","                labels,\n","                test_size=validation_split,\n","                random_state=42,\n","            )\n","\n","        print(f\"Training samples: {len(X_train)}\")\n","        print(f\"Validation samples: {len(X_val)}\")\n","\n","        # Tokenize data\n","        print(\"Tokenizing texts...\")\n","        train_encodings = self.tokenize_texts(X_train)\n","        val_encodings = self.tokenize_texts(X_val)\n","\n","        # Build model\n","        print(\"Building model...\")\n","        self.build_model()\n","\n","        # Set BERT layers to trainable (fine-tuning)\n","        if hasattr(self.model, 'bert_model'):\n","            for layer in self.model.bert_model.layers:\n","                layer.trainable = True\n","\n","        # Compile model\n","        optimizer = Adam(learning_rate=learning_rate)\n","        self.model.compile(\n","            optimizer=optimizer,\n","            loss='binary_crossentropy',\n","            metrics=['accuracy', 'precision', 'recall']\n","        )\n","\n","        # Print model summary\n","        print(\"\\nModel Architecture:\")\n","        self.model.summary()\n","\n","        # Calculate class weights\n","        if y_train.empty or len(np.unique(y_train)) < 1:\n","            class_weights_dict = {}\n","            print(\"Warning: y_train is empty or has no unique classes, cannot compute class weights.\")\n","        else:\n","            class_weights = compute_class_weight(\n","                'balanced',\n","                classes=np.unique(y_train),\n","                y=y_train\n","            )\n","            class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n","\n","        # Callbacks\n","        callbacks = [\n","            EarlyStopping(\n","                monitor='val_loss',\n","                patience=3,\n","                restore_best_weights=True,\n","                verbose=1\n","            ),\n","            ReduceLROnPlateau(\n","                monitor='val_loss',\n","                factor=0.5,\n","                patience=2,\n","                min_lr=1e-7,\n","                verbose=1\n","            )\n","        ]\n","\n","        # Prepare training data as tf.data.Dataset\n","        def create_dataset(encodings, labels, batch_size):\n","            def gen():\n","                for i in range(len(labels)):\n","                    yield {\n","                        'input_ids': encodings['input_ids'][i],\n","                        'attention_mask': encodings['attention_mask'][i]\n","                    }, labels.iloc[i]\n","\n","            dataset = tf.data.Dataset.from_generator(\n","                gen,\n","                output_signature=(\n","                    {\n","                        'input_ids': tf.TensorSpec(shape=(self.max_length,), dtype=tf.int32),\n","                        'attention_mask': tf.TensorSpec(shape=(self.max_length,), dtype=tf.int32)\n","                    },\n","                    tf.TensorSpec(shape=(), dtype=tf.int64)\n","                )\n","            )\n","            return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n","\n","        # Create datasets\n","        train_dataset = create_dataset(train_encodings, y_train, batch_size)\n","        val_dataset = create_dataset(val_encodings, y_val, batch_size) if not y_val.empty else None\n","\n","        # Train model\n","        print(f\"\\nStarting fine-tuning with {epochs} epochs...\")\n","        history = self.model.fit(\n","            train_dataset,\n","            validation_data=val_dataset,\n","            epochs=epochs,\n","            callbacks=callbacks,\n","            class_weight=class_weights_dict,\n","            verbose=1\n","        )\n","\n","        # Evaluate\n","        print(\"\\nEvaluating model...\")\n","        if val_dataset and not y_val.empty:\n","            val_predictions = self.model.predict(val_dataset)\n","            val_pred_binary = (val_predictions > 0.5).astype(int).flatten()\n","\n","            print(\"\\nValidation Results:\")\n","            print(classification_report(y_val, val_pred_binary))\n","            print(\"\\nConfusion Matrix:\")\n","            print(confusion_matrix(y_val, val_pred_binary))\n","\n","        return history\n","\n","    def predict(self, texts):\n","        \"\"\"Make predictions on new texts\"\"\"\n","        if self.model is None or self.tokenizer is None:\n","            print(\"Error: Model or tokenizer not loaded. Cannot make predictions.\")\n","            return []\n","\n","        if isinstance(texts, str):\n","            texts = [texts]\n","\n","        # Preprocess\n","        processed_texts = [self.preprocess_text(text, True) for text in texts]\n","        processed_texts_series = pd.Series(processed_texts)\n","\n","        # Tokenize\n","        if processed_texts_series.empty or all(p == \"\" for p in processed_texts):\n","            print(\"Warning: Input texts are empty or become empty after preprocessing.\")\n","            return []\n","\n","        encodings = self.tokenize_texts(processed_texts_series)\n","\n","        if encodings['input_ids'].shape[0] == 0:\n","            print(\"Warning: Tokenization resulted in empty inputs.\")\n","            return []\n","\n","        # Create prediction dataset\n","        def gen():\n","            for i in range(len(texts)):\n","                yield {\n","                    'input_ids': encodings['input_ids'][i],\n","                    'attention_mask': encodings['attention_mask'][i]\n","                }\n","\n","        predict_dataset = tf.data.Dataset.from_generator(\n","            gen,\n","            output_signature={\n","                'input_ids': tf.TensorSpec(shape=(self.max_length,), dtype=tf.int32),\n","                'attention_mask': tf.TensorSpec(shape=(self.max_length,), dtype=tf.int32)\n","            }\n","        ).batch(len(texts))\n","\n","        # Predict\n","        predictions = self.model.predict(predict_dataset)\n","\n","        # Format results\n","        results = []\n","        for i, text in enumerate(texts):\n","            if i < len(predictions):\n","                confidence = float(predictions[i][0])\n","                is_hoax = confidence > 0.5\n","\n","                results.append({\n","                    'text': text,\n","                    'prediction': 'HOAX' if is_hoax else 'NOT HOAX',\n","                    'confidence': confidence,\n","                    'is_hoax': is_hoax\n","                })\n","            else:\n","                results.append({\n","                    'text': text,\n","                    'prediction': 'Error',\n","                    'confidence': 0.0,\n","                    'is_hoax': False\n","                })\n","\n","        return results\n","\n","    def save_model(self, model_path='/content/drive/MyDrive/PengolahanData/Hasil_Modelling/indobert_hoax_model', tokenizer_path='/content/drive/MyDrive/PengolahanData/Hasil_Modelling/tokenizer_config.json'):\n","        \"\"\"Save fine tuned model\"\"\"\n","        if self.model:\n","          #Ensure the model_path ends with .weights.h5\n","          if not model_path.endswith('.weights.h5'):\n","            model_path += '.weights.h5'\n","\n","          self.model.save_weights(model_path)\n","\n","          # Save tokenizer configuration\n","          tokenizer_config = {\n","             'model_name': self.model_name,\n","             'max_length': self.max_length\n","            }\n","\n","          with open(tokenizer_path, 'w') as f:\n","              json.dump(tokenizer_config, f)\n","\n","          print(f\"Model weights saved to {model_path}\")\n","          print(f\"Tokenizer config saved to {tokenizer_path}\")\n","        else:\n","            print(\"Error: No model to save.\")\n","\n","    def load_model(self, model_path='/content/drive/MyDrive/PengolahanData/Hasil_Modelling/indobert_hoax_model', tokenizer_path='/content/drive/MyDrive/PengolahanData/Hasil_Modelling/tokenizer_config.json'):\n","        \"\"\"Load fine tuned model\"\"\"\n","        try:\n","            # Load tokenizer config\n","            with open(tokenizer_path, 'r') as f:\n","                tokenizer_config = json.load(f)\n","\n","            self.model_name = tokenizer_config['model_name']\n","            self.max_length = tokenizer_config['max_length']\n","\n","            # Reinitialize tokenizer\n","            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n","\n","            # Build model architecture\n","            self.build_model()\n","\n","            # Load weights\n","            self.model.load_weights(model_path)\n","\n","            print(\"Model and tokenizer loaded successfully!\")\n","        except FileNotFoundError as e:\n","            print(f\"Error loading model/tokenizer: {e}\")\n","            self.model = None\n","            self.tokenizer = None\n","        except Exception as e:\n","            print(f\"An unexpected error occurred while loading the model: {e}\")\n","            self.model = None\n","            self.tokenizer = None"],"metadata":{"id":"cpsmqHWrEcap","executionInfo":{"status":"ok","timestamp":1749795000894,"user_tz":-420,"elapsed":40,"user":{"displayName":"ISNAN WIJAYAKUSUMA","userId":"13733359583425132025"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Simplified Inference Class for IndoBERT\n","class IndoBERTInference:\n","    def __init__(self, model_path='/content/drive/MyDrive/PengolahanData/Hasil_Modelling/indobert_hoax_model', tokenizer_path='/content/drive/MyDrive/PengolahanData/Hasil_Modelling/tokenizer_config.json'):\n","        self.model = None\n","        self.tokenizer = None\n","        self.max_length = None\n","        self.load_model(model_path, tokenizer_path)\n","\n","    def load_model(self, model_path, tokenizer_path):\n","        \"\"\"Load Trained model\"\"\"\n","        try:\n","            # Load config\n","            with open(tokenizer_path, 'r') as f:\n","                config = json.load(f)\n","\n","            self.max_length = config['max_length']\n","\n","            # Load tokenizer\n","            self.tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n","\n","            # Recreate model architecture (same as in main class)\n","            class IndoBERTClassifier(tf.keras.Model):\n","                def __init__(self, model_name, max_length, **kwargs):\n","                    super(IndoBERTClassifier, self).__init__(**kwargs)\n","                    self.max_length = max_length\n","                    self.bert_model = TFAutoModel.from_pretrained(model_name)\n","                    self.dropout1 = layers.Dropout(0.3)\n","                    self.dense1 = layers.Dense(256, activation='relu', name='dense_1')\n","                    self.dropout2 = layers.Dropout(0.2)\n","                    self.dense2 = layers.Dense(128, activation='relu', name='dense_2')\n","                    self.dropout3 = layers.Dropout(0.1)\n","                    self.classifier = layers.Dense(1, activation='sigmoid', name='classifier')\n","\n","                def call(self, inputs, training=None):\n","                    input_ids = inputs['input_ids']\n","                    attention_mask = inputs['attention_mask']\n","\n","                    bert_output = self.bert_model(\n","                        input_ids=input_ids,\n","                        attention_mask=attention_mask,\n","                        training=training\n","                    )\n","\n","                    pooled_output = bert_output.last_hidden_state[:, 0, :]\n","\n","                    x = self.dropout1(pooled_output, training=training)\n","                    x = self.dense1(x)\n","                    x = self.dropout2(x, training=training)\n","                    x = self.dense2(x)\n","                    x = self.dropout3(x, training=training)\n","                    outputs = self.classifier(x)\n","\n","                    return outputs\n","\n","            # Create and build model\n","            self.model = IndoBERTClassifier(\n","                model_name=config['model_name'],\n","                max_length=self.max_length,\n","                name='IndoBERT_Classifier'\n","            )\n","\n","            # Build with dummy input\n","            dummy_input = {\n","                'input_ids': tf.zeros((1, self.max_length), dtype=tf.int32),\n","                'attention_mask': tf.zeros((1, self.max_length), dtype=tf.int32)\n","            }\n","            _ = self.model(dummy_input)\n","\n","            if not model_path.endswith('.weights.h5'):\n","                model_path += '.weights.h5'\n","\n","            # Load weights\n","            self.model.load_weights(model_path)\n","\n","            print(\"IndoBERT model loaded successfully ✓\")\n","        except FileNotFoundError as e:\n","            print(f\"❌ Error loading model: {e}\")\n","            self.model = None\n","            self.tokenizer = None\n","        except Exception as e:\n","            print(f\"❌ An unexpected error occurred while loading model: {str(e)}\")\n","            self.model = None\n","            self.tokenizer = None\n","\n","    def predict(self, text):\n","        \"\"\"Simple prediction function\"\"\"\n","        if self.model is None or self.tokenizer is None:\n","            print(\"Error: Model or tokenizer not loaded. Cannot make predictions.\")\n","            return {'text': text, 'prediction': 'Error', 'confidence': 0.0, 'is_hoax': False}\n","\n","        # Tokenize\n","        encoded = self.tokenizer(\n","            [text],\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='tf',\n","            return_attention_mask=True\n","        )\n","\n","        # Predict\n","        predictions = self.model({\n","            'input_ids': encoded['input_ids'],\n","            'attention_mask': encoded['attention_mask']\n","        })\n","\n","        if predictions.shape[0] > 0:\n","            confidence = float(predictions[0][0])\n","            is_hoax = confidence > 0.5\n","        else:\n","            confidence = 0.0\n","            is_hoax = False\n","\n","        return {\n","            'text': text,\n","            'prediction': 'HOAX' if is_hoax else 'NOT HOAX',\n","            'confidence': confidence,\n","            'is_hoax': is_hoax\n","        }"],"metadata":{"id":"K72aMJEKi56I","executionInfo":{"status":"ok","timestamp":1749795001083,"user_tz":-420,"elapsed":186,"user":{"displayName":"ISNAN WIJAYAKUSUMA","userId":"13733359583425132025"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Main execution\n","if __name__ == \"__main__\":\n","    # Initialize classifier\n","    classifier = IndoBERTHoaxClassifier(\n","        model_name='indobenchmark/indobert-base-p1',\n","        max_length=128\n","    )\n","\n","    # Train the model (fine tuning)\n","    print(\"Fine-tuning IndoBERT for hoax classification...\")\n","    history = classifier.train(\n","        csv_path='/content/drive/MyDrive/PengolahanData/Data_Fix/preprocessed_text_label.csv',\n","        validation_split=0.2,\n","        epochs=3,\n","        batch_size=16,\n","        learning_rate=2e-5\n","    )\n","\n","    # Only save model if training was successful\n","    if history is not None and classifier.model is not None:\n","        classifier.save_model()\n","    else:\n","        print(\"Skipping model save due to training failure or empty dataset.\")\n","\n","    # Test predictions\n","    test_texts = [\n","        \"Pemerintah akan memberikan bantuan langsung tunai kepada seluruh rakyat Indonesia\",\n","        \"Menteri Kesehatan mengumumkan kebijakan baru terkait protokol kesehatan\",\n","        \"Vaksin COVID-19 berbahaya dan dapat mengubah DNA manusia secara permanen\",\n","        \"\",\n","        \"Pendek text\"\n","    ]\n","\n","    print(\"\\nTesting IndoBERT predictions:\")\n","    if classifier.model is not None:\n","        predictions = classifier.predict(test_texts)\n","        for pred in predictions:\n","            if pred['text']:  # Only show non-empty texts\n","                print(f\"\\nText: {pred['text'][:80]}...\")\n","                print(f\"Prediction: {pred['prediction']}\")\n","                print(f\"Confidence: {pred['confidence']:.3f}\")\n","                print(f\"Is Hoax: {pred['is_hoax']}\")\n","    else:\n","        print(\"Cannot make predictions: Model is not available.\")"],"metadata":{"id":"sqU6XTXTjAb6","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a7c7d308454a4b62841f10eaf5b62251","5b64ff15803c4fdbb8f3e0063aa3d556","304817cd51f34112b8a638c64eae240d","a101c9a36d584b8f9bb74b5391738f47","01907a7d96c645cd87b3cb1ed2afbecd","544034fcd88a4814a03cb7354240dff0","68300ee3fa684a5e83b210bdf2fa3534","d8f37db40cb44ed1adf9e0f1aa2d96b4","853e7be233c2408590377373932f6d30","a9b01b2ab9be4717b93a8279d8283207","5456187ed2834838853faa0705fcfcfc","a32ce732bdf94eedba46bbb7f4e95180","eeb6c3edb4a54288a3aae07c3b2aedac","8aad0229d9e044c7b0ae3a7a0f5e5544","579fa3d14aac422db5f4abf5fb904b8b","e1e9731a30c94545a6707e0c4e973708","68749d33967f452dbc19c78de79cac40","c17d9c43744a48f9a832f71b7eaff513","41ba949426c2475f9dceec60db6b07cb","c5b4731d282d4767998a4d3044ff826e","618d0e9d6d9c47ddae3c8ab2ba4a5d53","a20705ec11f24557a176ab1bbcdd5547","a29d38b114fa48a3b3ddc781ad14eec8","badb1228d20049288101cb6152539e06","7c43704a4d3d433496a4dfd7aa59a540","50bfef63237a4a5b87232cf68b2df515","0866f1eff30446e280e0a2a7f5547404","b36c1f83c1624641b7307e20f4509d79","15452b96a92c4d0fb4d7cb4b712af51e","b0292e26b7ac4e24a5fcbb35df1f8241","f2918279ecca4c8ebffcbab92ed8e230","da460b6706eb4bef9c1a6216697450c5","f5356351b3db4123aa5c49248e829bb5","272ebbba3b2045f490b4f6ca44a168f8","fea31b9cc7014200a97acd3b257a5ff0","317e81a28cf84116bb1f7b599c0e085e","d5a22dd934d645df811f173b741ef273","fa2edb46309c47adaf6ce87fffbd243a","23b32784f98f4892bc40cec5524dbcf3","cfd9e2f60e2e4621bdee13fd20dda261","a82dab2de32a43b5a1f64762a5f8adb8","db3e5d7440de43f58592ce3b64272aca","08973217de124726b42c6ef576aa6376","499367cbb5fe42caa03c4201384ab8e4","cfd81693b04d449b8682f5f7e8e4834c","b9633043a6ea43d4a80d7079110b9ae8","6d550dd5d73a4611a901a30e5f6b8f46","d4a4bdee6da346e081a7a046c8a7d813","d1e965612121401fbbd736937b3be73a","c626689b46164abbb11022c7da16d9e8","afc4a499dcc64998ada0a9ebe2be5791","2dbf9c303e594cffbc8bd7c7bdbd7440","b3b3b826a5a947de89ab614dae170bc4","4a4e607e49434677897065ca4e11fd7a","f08e00e5d2994daa9c47f7d0b5495112"]},"executionInfo":{"status":"ok","timestamp":1749795569580,"user_tz":-420,"elapsed":306684,"user":{"displayName":"ISNAN WIJAYAKUSUMA","userId":"13733359583425132025"}},"outputId":"16fe3492-38b7-4a21-d968-6eb03bb785c0"},"execution_count":4,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a7c7d308454a4b62841f10eaf5b62251","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a32ce732bdf94eedba46bbb7f4e95180","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a29d38b114fa48a3b3ddc781ad14eec8","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"272ebbba3b2045f490b4f6ca44a168f8","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Fine-tuning IndoBERT for hoax classification...\n","Preparing data\n","Loading data from /content/drive/MyDrive/PengolahanData/Data_Fix/preprocessed_text_label.csv\n","Original dataset shape: (31678, 2)\n","Shape after dropping NA: (31660, 2)\n","Dataset distribution before balancing:\n","label\n","0    21710\n","1     9950\n","Name: count, dtype: int64\n","Dataset distribution after balancing:\n","label\n","0    9950\n","1    9950\n","Name: count, dtype: int64\n","Shape after balancing: (19900, 2)\n","Shape after preprocessing text: (19900, 2)\n","Shape after removing empty texts: (19895, 2)\n","Final dataset shape: (19895, 2)\n","Training samples: 15916\n","Validation samples: 3979\n","Tokenizing texts...\n","Building model...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cfd81693b04d449b8682f5f7e8e4834c","version_major":2,"version_minor":0},"text/plain":["tf_model.h5:   0%|          | 0.00/656M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at indobenchmark/indobert-base-p1 were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at indobenchmark/indobert-base-p1.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Model Architecture:\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"IndoBERT_Classifier\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"IndoBERT_Classifier\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               │       <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)               │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ classifier (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)               │       \u001b[38;5;34m196,864\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)               │        \u001b[38;5;34m32,896\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ classifier (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │           \u001b[38;5;34m129\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">229,889</span> (898.00 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m229,889\u001b[0m (898.00 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">229,889</span> (898.00 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m229,889\u001b[0m (898.00 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Starting fine-tuning with 3 epochs...\n","Epoch 1/3\n","    995/Unknown \u001b[1m161s\u001b[0m 132ms/step - accuracy: 0.7375 - loss: 0.5254 - precision: 0.7513 - recall: 0.6912"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n","  self._interrupted_warning()\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m995/995\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 166ms/step - accuracy: 0.7376 - loss: 0.5253 - precision: 0.7514 - recall: 0.6913 - val_accuracy: 0.9193 - val_loss: 0.2349 - val_precision: 0.9108 - val_recall: 0.9296 - learning_rate: 2.0000e-05\n","Epoch 2/3\n","\u001b[1m995/995\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 140ms/step - accuracy: 0.8898 - loss: 0.2877 - precision: 0.8824 - recall: 0.8972 - val_accuracy: 0.9264 - val_loss: 0.2060 - val_precision: 0.9240 - val_recall: 0.9291 - learning_rate: 2.0000e-05\n","Epoch 3/3\n","\u001b[1m995/995\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 139ms/step - accuracy: 0.9065 - loss: 0.2504 - precision: 0.8985 - recall: 0.9147 - val_accuracy: 0.9294 - val_loss: 0.1928 - val_precision: 0.9257 - val_recall: 0.9336 - learning_rate: 2.0000e-05\n","Restoring model weights from the end of the best epoch: 3.\n","\n","Evaluating model...\n","\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 130ms/step\n","\n","Validation Results:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.93      0.93      1990\n","           1       0.93      0.93      0.93      1989\n","\n","    accuracy                           0.93      3979\n","   macro avg       0.93      0.93      0.93      3979\n","weighted avg       0.93      0.93      0.93      3979\n","\n","\n","Confusion Matrix:\n","[[1841  149]\n"," [ 132 1857]]\n","Model weights saved to /content/drive/MyDrive/PengolahanData/Hasil_Modelling/indobert_hoax_model.weights.h5\n","Tokenizer config saved to /content/drive/MyDrive/PengolahanData/Hasil_Modelling/tokenizer_config.json\n","\n","Testing IndoBERT predictions:\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n","\n","Text: Pemerintah akan memberikan bantuan langsung tunai kepada seluruh rakyat Indonesi...\n","Prediction: HOAX\n","Confidence: 0.888\n","Is Hoax: True\n","\n","Text: Menteri Kesehatan mengumumkan kebijakan baru terkait protokol kesehatan...\n","Prediction: HOAX\n","Confidence: 0.686\n","Is Hoax: True\n","\n","Text: Vaksin COVID-19 berbahaya dan dapat mengubah DNA manusia secara permanen...\n","Prediction: HOAX\n","Confidence: 0.953\n","Is Hoax: True\n","\n","Text: Pendek text...\n","Prediction: HOAX\n","Confidence: 0.992\n","Is Hoax: True\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n","  self._interrupted_warning()\n"]}]},{"cell_type":"code","source":["texts = [\n","    \"Jokowi akan digantikan oleh alien dalam upacara kenegaraan minggu depan.\",\n","    \"Presiden mengunjungi nelayan di Bitung untuk menyerahkan bantuan alat tangkap.\",\n","    \"juru bicara klaim prabowo percaya maju pilpres restu jokowi juru bicara menteri pertahanan prabowo subianto dahnil anzar simanjuntak prabowo percaya maju calon presiden capres pilpres restu presiden joko widodo jokowi dahnil restu suara dukungan prabowo bertambah restu restu jokowi prabowo semangat pemilih prabowo bertambah dahnil acara political show cnn indonesia tv senin malam dahnil prabowo salah tokoh memiliki adab capres prabowo izin jokowi melenggang kontestasi politik prabowo agenda politik lakukan mengganggu kinerja tugastugasnya menteri pertahanan kepemimpinan jokowi prabowo beliau jokowi silahkan mengizinkan prabowo proses kontestasi dahnil gerindra mendukung sepenuhnya pencapresan prabowo suara grass root gerindra sambungnya jokowi memperkenalkan tokoh berpotensi capres cawapres pilpres peringatan ulang partai persatuan pembangunan ppp jumat salah tokoh jokowi prabowo jokowi menyinggung kans prabowo calon presiden pilpres mengungkit reputasi kemenangan prabowo pilpres kali pilpres menang mohon maaf prabowo jatahnya prabowo jokowi puncak peringatan ulang perindo jakarta senin lnadal\"\n","]\n","\n","for text in texts:\n","    result = classifier.predict(text)\n","    print(f\"Teks: {text}\\nHasil: {result}\\n\")"],"metadata":{"id":"B2SAPZJ_mJhV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749795574686,"user_tz":-420,"elapsed":5094,"user":{"displayName":"ISNAN WIJAYAKUSUMA","userId":"13733359583425132025"}},"outputId":"783a25f5-f3c5-4e01-99c4-4f4ee5d3e963"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n","Teks: Jokowi akan digantikan oleh alien dalam upacara kenegaraan minggu depan.\n","Hasil: [{'text': 'Jokowi akan digantikan oleh alien dalam upacara kenegaraan minggu depan.', 'prediction': 'HOAX', 'confidence': 0.7233927845954895, 'is_hoax': True}]\n","\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n","Teks: Presiden mengunjungi nelayan di Bitung untuk menyerahkan bantuan alat tangkap.\n","Hasil: [{'text': 'Presiden mengunjungi nelayan di Bitung untuk menyerahkan bantuan alat tangkap.', 'prediction': 'HOAX', 'confidence': 0.918043315410614, 'is_hoax': True}]\n","\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n","Teks: juru bicara klaim prabowo percaya maju pilpres restu jokowi juru bicara menteri pertahanan prabowo subianto dahnil anzar simanjuntak prabowo percaya maju calon presiden capres pilpres restu presiden joko widodo jokowi dahnil restu suara dukungan prabowo bertambah restu restu jokowi prabowo semangat pemilih prabowo bertambah dahnil acara political show cnn indonesia tv senin malam dahnil prabowo salah tokoh memiliki adab capres prabowo izin jokowi melenggang kontestasi politik prabowo agenda politik lakukan mengganggu kinerja tugastugasnya menteri pertahanan kepemimpinan jokowi prabowo beliau jokowi silahkan mengizinkan prabowo proses kontestasi dahnil gerindra mendukung sepenuhnya pencapresan prabowo suara grass root gerindra sambungnya jokowi memperkenalkan tokoh berpotensi capres cawapres pilpres peringatan ulang partai persatuan pembangunan ppp jumat salah tokoh jokowi prabowo jokowi menyinggung kans prabowo calon presiden pilpres mengungkit reputasi kemenangan prabowo pilpres kali pilpres menang mohon maaf prabowo jatahnya prabowo jokowi puncak peringatan ulang perindo jakarta senin lnadal\n","Hasil: [{'text': 'juru bicara klaim prabowo percaya maju pilpres restu jokowi juru bicara menteri pertahanan prabowo subianto dahnil anzar simanjuntak prabowo percaya maju calon presiden capres pilpres restu presiden joko widodo jokowi dahnil restu suara dukungan prabowo bertambah restu restu jokowi prabowo semangat pemilih prabowo bertambah dahnil acara political show cnn indonesia tv senin malam dahnil prabowo salah tokoh memiliki adab capres prabowo izin jokowi melenggang kontestasi politik prabowo agenda politik lakukan mengganggu kinerja tugastugasnya menteri pertahanan kepemimpinan jokowi prabowo beliau jokowi silahkan mengizinkan prabowo proses kontestasi dahnil gerindra mendukung sepenuhnya pencapresan prabowo suara grass root gerindra sambungnya jokowi memperkenalkan tokoh berpotensi capres cawapres pilpres peringatan ulang partai persatuan pembangunan ppp jumat salah tokoh jokowi prabowo jokowi menyinggung kans prabowo calon presiden pilpres mengungkit reputasi kemenangan prabowo pilpres kali pilpres menang mohon maaf prabowo jatahnya prabowo jokowi puncak peringatan ulang perindo jakarta senin lnadal', 'prediction': 'NOT HOAX', 'confidence': 0.00192004325799644, 'is_hoax': False}]\n","\n"]}]}]}